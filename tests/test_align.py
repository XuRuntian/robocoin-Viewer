import sys
import os
import numpy as np
import cv2
import json
from PIL import Image
from sklearn.cluster import KMeans  # å¼•å…¥ K-Means è¿›è¡Œç‰©ç†çŠ¶æ€èšç±»
from sklearn.preprocessing import StandardScaler

# ç¡®ä¿èƒ½å¯¼å…¥ src æ¨¡å—
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from src.adapters.lerobot_adapter import LeRobotAdapter

class KinematicScreener:
    """è¿åŠ¨å­¦ç­›é€‰å™¨ï¼šåŸºäºåº•å±‚æ•°æ®å¯»æ‰¾é«˜èƒ½æ´»è·ƒåŒºå’Œä»£è¡¨æ€§å…³é”®å¸§"""
    def __init__(self, fps=30):
        self.fps = fps

    def compute_energy(self, qpos_data):
        # è®¡ç®—ä¸€é˜¶å¯¼æ•°ï¼ˆé€Ÿåº¦ï¼‰ï¼Œæ±‚å¹³æ–¹å’Œä»£è¡¨åŠ¨èƒ½
        velocity = np.diff(qpos_data, axis=0, prepend=qpos_data[0:1])
        energy = np.sum(velocity ** 2, axis=1)
        return energy

    def get_active_window(self, qpos_data, window_size=15, sensitivity=2.0):
        """å¯»æ‰¾å»é™¤å‘å‘†æ—¶é—´çš„çœŸæ­£æ´»è·ƒåŒºé—´"""
        raw_energy = self.compute_energy(qpos_data)
        window = np.ones(window_size) / window_size
        smooth_energy = np.convolve(raw_energy, window, mode='same')
        
        # åŠ¨æ€è®¡ç®—åº•å™ª (å–æœ€å®‰é™çš„ 5% çš„å¸§çš„å¹³å‡èƒ½é‡)
        noise_floor = np.mean(np.sort(smooth_energy)[:int(max(1, len(smooth_energy)*0.05))]) 
        noise_floor = max(noise_floor, 1e-6) 
        threshold = noise_floor * sensitivity
        
        active_indices = np.where(smooth_energy > threshold)[0]
        if len(active_indices) == 0:
            return 0, len(qpos_data) - 1

        start_frame = active_indices[0]
        end_frame = active_indices[-1]
        
        # å®‰å…¨ä½™é‡ Padding (å‰åå„å¤šç»™ 0.5 ç§’ï¼Œç»™ VLM æä¾›èµ·æ‰‹å¼ä¸Šä¸‹æ–‡)
        padding = int(0.5 * self.fps)
        start_frame = max(0, start_frame - padding)
        end_frame = min(len(qpos_data) - 1, end_frame + padding)
        print(f"ğŸ” è¿åŠ¨å­¦ç­›é€‰: æ´»è·ƒåŒºé—´ [{start_frame} -> {end_frame}] (èƒ½é‡é˜ˆå€¼: {threshold:.4f})")
        return start_frame, end_frame

    def select_key_frames_kmeans(self, qpos_data, active_start, active_end, num_frames=9):
        """å‡çº§ç‰ˆï¼šèåˆæ—¶é—´åµŒå…¥ä¸é€Ÿåº¦æƒé‡çš„ K-Means ç‰©ç†çŠ¶æ€èšç±»"""
        active_qpos = qpos_data[active_start:active_end+1]
        
        if len(active_qpos) < num_frames:
            return np.linspace(active_start, active_end, num_frames, dtype=int).tolist()
            
        # 1. åŸºç¡€ç‰¹å¾ï¼šä½ç½®ç‰¹å¾å½’ä¸€åŒ–
        scaler_pos = StandardScaler()
        qpos_scaled = scaler_pos.fit_transform(active_qpos)
        # 2. åŠ¨åŠ¿ç‰¹å¾ï¼šé€Ÿåº¦ç‰¹å¾å½’ä¸€åŒ–
        velocities = np.diff(active_qpos, axis=0, prepend=active_qpos[0:1])
        scaler_vel = StandardScaler()
        vel_scaled = scaler_vel.fit_transform(velocities)
        
        # 3. æ—¶åºç‰¹å¾ï¼šTemporal Embedding
        time_steps = np.arange(len(active_qpos)).reshape(-1, 1)
        scaler_time = StandardScaler()
        time_scaled = scaler_time.fit_transform(time_steps)
        
        # 4. ğŸ§  æ ¸å¿ƒé­”æ³•ï¼šå¤šæ¨¡æ€ç‰¹å¾åŠ æƒæ‹¼æ¥
        W_pos = 1.0   
        W_vel = 2.0   
        W_time = 1.5  
        
        features = np.hstack([
            qpos_scaled * W_pos, 
            vel_scaled * W_vel, 
            time_scaled * W_time
        ])
        
        # 5. æ‰§è¡Œèšç±»
        kmeans = KMeans(n_clusters=num_frames, random_state=42, n_init=10)
        kmeans.fit(features)
        
        # 6. å¯»æ‰¾æœ€è´´è¿‘èšç±»ä¸­å¿ƒçš„çœŸå®å¸§
        key_indices = []
        for center in kmeans.cluster_centers_:
            distances = np.linalg.norm(features - center, axis=1)
            closest_idx = np.argmin(distances) + active_start
            key_indices.append(closest_idx)
            
        key_indices = sorted(list(set(key_indices)))
        
        while len(key_indices) < num_frames:
            fallback = np.linspace(active_start, active_end, num_frames, dtype=int).tolist()
            key_indices = sorted(list(set(key_indices + fallback)))[:num_frames]
            
        return key_indices

# ==========================================
# ğŸ‘‡ æ³›ç”¨å‹å¾®è§‚ç‰©ç†å¯¹é½å¼•æ“
# ==========================================
def find_exact_transition_frame(qpos_window, global_start_idx, gripper_dim_indices, gripper_threshold=0.02):
    """
    åº•å±‚å°è„‘ï¼šå¯»æ‰¾ç²¾ç¡®çš„åŠ¨ä½œåˆ‡æ¢å¸§ (æ”¯æŒçµå·§æ‰‹ä¸å•/åŒç»´å¤¹çˆª)
    åŸºäºæ‰€æœ‰æœ«ç«¯ç»´åº¦çš„å¤åˆé€Ÿåº¦ (L2 Norm) æ¥æ„ŸçŸ¥åŠ¨ä½œçªå˜ã€‚
    """
    if len(qpos_window) < 3 or not gripper_dim_indices:
        return global_start_idx + len(qpos_window) // 2

    # è¿‡æ»¤æ‰è¶Šç•Œçš„ç»´åº¦ï¼Œé˜²æ­¢æŠ¥é”™
    valid_gripper_dims = [d for d in gripper_dim_indices if -qpos_window.shape[1] <= d < qpos_window.shape[1]]
    if not valid_gripper_dims:
        return global_start_idx + len(qpos_window) // 2

    # ç­–ç•¥ A: å¯»æ‰¾æœ«ç«¯å¤åˆåŠ¨ä½œçªå˜ç‚¹ (æ”¯æŒçµå·§æ‰‹ååŒè¿åŠ¨)
    # 1. æå–æ‰€æœ‰æœ«ç«¯ç»´åº¦çš„çŠ¶æ€
    gripper_data = qpos_window[:, valid_gripper_dims]
    
    # 2. è®¡ç®—æœ«ç«¯æ•´ä½“çš„å¤åˆé€Ÿåº¦ (L2èŒƒæ•°)
    gripper_diff = np.diff(gripper_data, axis=0)
    composite_velocity = np.linalg.norm(gripper_diff, axis=1)
    
    if len(composite_velocity) > 0:
        max_change_idx = np.argmax(composite_velocity)
        max_change_val = composite_velocity[max_change_idx]

        if max_change_val > gripper_threshold:
            return global_start_idx + max_change_idx

    # ç­–ç•¥ B: å¦‚æœæœ«ç«¯æ²¡åŠ¨ï¼Œå¯»æ‰¾æ‰‹è‡‚åŠ¨èƒ½æå°å€¼ç‚¹ (åŠ¨ä½œåœé¡¿/è¿‡æ¸¡ç‚¹)
    all_dims = set(range(qpos_window.shape[1]))
    arm_dims = list(all_dims - set(valid_gripper_dims))
    
    if not arm_dims: # æç«¯æƒ…å†µï¼šå…¨æ˜¯æœ«ç«¯ç»´åº¦
        return global_start_idx + len(qpos_window) // 2

    arm_qpos = qpos_window[:, arm_dims]
    arm_velocity = np.diff(arm_qpos, axis=0)
    energy = np.sum(arm_velocity ** 2, axis=1)
    
    if len(energy) > 0:
        bottleneck_idx = np.argmin(energy)
        return global_start_idx + bottleneck_idx
    
    return global_start_idx + len(qpos_window) // 2

def align_and_segment(vlm_json, indices_rel, qpos_data, dataset_start_offset, gripper_dim_indices, gripper_threshold):
    """å°† VLM çš„å®è§‚ JSON å¯¹é½åˆ°åº•å±‚æ•°æ®"""
    final_annotations = []
    print("\nğŸš€ å¼€å§‹è¿›è¡Œç‰©ç†-è¯­ä¹‰ç¼åˆ...")
    
    for i, task in enumerate(vlm_json):
        img_start_idx = task["start_image"] - 1
        img_end_idx = task["end_image"] - 1
        
        # ä½¿ç”¨ K-Means è¾“å‡ºçš„ç›¸å¯¹ç´¢å¼•
        rough_frame_start = indices_rel[img_start_idx]
        rough_frame_end = indices_rel[img_end_idx]
        
        # åˆ‡å‰²å¯¹åº”çš„åº•å±‚ç‰©ç†æ•°æ®
        window_qpos = qpos_data[rough_frame_start : rough_frame_end + 1]
        
        # ç²¾æœåˆ‡å‰²ç‚¹ï¼Œä¼ å…¥æŒ‡å®šçš„æœ«ç«¯ç»´åº¦å’Œé˜ˆå€¼
        exact_end = find_exact_transition_frame(
            window_qpos, 
            global_start_idx=rough_frame_start,
            gripper_dim_indices=gripper_dim_indices,
            gripper_threshold=gripper_threshold
        )
        
        if i == 0:
            exact_start = rough_frame_start
        else:
            exact_start = final_annotations[-1]["exact_end_frame_relative"]
            
        global_start = dataset_start_offset + exact_start
        global_end = dataset_start_offset + exact_end
        
        final_annotations.append({
            "subtask_id": task["subtask_id"],
            "instruction": task["instruction"],
            "exact_start_frame_relative": int(exact_start),
            "exact_end_frame_relative": int(exact_end),
            "global_start_frame": int(global_start),
            "global_end_frame": int(global_end)
        })
        
        print(f"âœ… å­ä»»åŠ¡ {task['subtask_id']}: {task['instruction']}")
        print(f"   VLM å®šç•Œ: å›¾ {task['start_image']} -> {task['end_image']} (é¢„ä¼°: {dataset_start_offset+rough_frame_start} -> {dataset_start_offset+rough_frame_end})")
        print(f"   ğŸ”ª ç²¾å‡†åˆ‡åˆ†: ç¬¬ {global_start} å¸§ -> ç¬¬ {global_end} å¸§\n")

    return final_annotations

# ==========================================
# ğŸ‘‡ å›¾åƒæ’ç‰ˆå·¥å…·
# ==========================================
def draw_text_cv2(img_array, text, position=(20, 50), font_scale=1.5, thickness=3, text_color=(255, 255, 255)):
    img = img_array.copy()
    (tw, th), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)
    x, y = position
    cv2.rectangle(img, (x - 10, y - th - 10), (x + tw + 10, y + 10), (0, 0, 0), -1)
    cv2.putText(img, text, position, cv2.FONT_HERSHEY_SIMPLEX, font_scale, text_color, thickness)
    return img

def create_optimal_composite_frame(frame_images, step_num):
    global_cam = None
    for key in ['cam_high_rgb', 'cam_third_view', 'front']:
        if key in frame_images and frame_images[key] is not None:
            global_cam = frame_images[key]
            break
    if global_cam is None:
        global_cam = next(iter(frame_images.values()))

    local_cam = None
    for key in ['cam_right_wrist_rgb', 'cam_left_wrist_rgb', 'wrist']:
        if key in frame_images and frame_images[key] is not None:
            local_cam = frame_images[key]
            break
    if local_cam is None:
        local_cam = global_cam  

    target_size = (640, 480)
    global_cam = cv2.resize(global_cam, target_size)
    local_cam = cv2.resize(local_cam, target_size)

    global_cam = draw_text_cv2(global_cam, f"[{step_num}] Global", position=(20, 60), text_color=(255, 50, 50))
    local_cam = draw_text_cv2(local_cam, f"Wrist", position=(20, 60), text_color=(255, 255, 255))

    return np.vstack((global_cam, local_cam))

# ==========================================
# ğŸ‘‡ ä¸»å‡½æ•°ï¼šRobo-ETL æ ¸å¿ƒç®¡çº¿
# ==========================================
def main():
    # --- ğŸ¤– ç¡¬ä»¶å‚æ•°é…ç½®åŒº ---
    # æ ¹æ®ä½ çš„æœºå™¨äººæœ«ç«¯ç±»å‹åœ¨æ­¤æ‰‹åŠ¨æŒ‡å®šç»´åº¦å’Œé˜ˆå€¼
    ROBOT_CONFIG = {
        # ç¤ºä¾‹ A (å•è‡‚/åŒè‡‚ç®€å•å¤¹çˆª): [-1] æˆ– [-1, -2]
        # ç¤ºä¾‹ B (çµå·§æ‰‹): [14, 15, 16, 17, 18, 19, 20]
        "gripper_dim_indices": list(range(12, 36)),
        
        # åŠ¨ä½œåˆ¤å®šé˜ˆå€¼: çµå·§æ‰‹ç”±äºå•å…³èŠ‚ä½ç§»å°ï¼Œå»ºè®®é™ä½åˆ° 0.02; ç®€å•å¤¹çˆªå¯ä¿æŒ 0.05
        "gripper_threshold": 0.05        
    }
    # --------------------------

    dataset_path = "/home/shwu/xrt/test_data/AIRBOT_MMK2_mobile_phone_storage/"
    reader = LeRobotAdapter()
    if not reader.load(dataset_path):
        return
        
    total_length = reader.get_length()
    print(f"ğŸ“Š æ•°æ®é›†æ€»å¸§æ•°: {total_length}")
    
    EP_START = 0
    EP_END = min(194, total_length - 1) 
    
    print("ğŸ” æ­£åœ¨æå–å½“å‰ Episode çš„åº•å±‚ç‰©ç†æ•°æ®...")
    qpos_list = []
    
    for idx in range(EP_START, EP_END + 1):
        frame = reader.get_frame(idx)
        state = getattr(frame, 'state', {})
        val = state.get("qpos")
        if val is None:
            val = state.get("action") 
        if val is not None:
            qpos_list.append(val)
        else:
            # å…œåº•ï¼šå¦‚æœéƒ½æ²¡æœ‰ï¼Œç»™ä¸ª 0 å‘é‡é˜²æ­¢å´©æºƒ
            qpos_list.append(np.zeros(6))
        
    qpos_data = np.array(qpos_list) # shape: (EP_Length, Dims)
    
    screener = KinematicScreener(fps=30)
    active_start_rel, active_end_rel = screener.get_active_window(qpos_data, window_size=15, sensitivity=2.0)
    
    print("ğŸ§  æ­£åœ¨è¿›è¡Œ K-Means ç‰©ç†çŠ¶æ€èšç±»...")
    indices_rel = screener.select_key_frames_kmeans(qpos_data, active_start_rel, active_end_rel, num_frames=9)
    indices = [EP_START + idx for idx in indices_rel]
    
    print(f"âœ‚ï¸ åŸå§‹ç²—æš´æˆªæ–­: [0 -> {EP_END}]")
    print(f"ğŸ¯ ç‰©ç†å¼•æ“åŠ¨æ€é”å®š: æ´»è·ƒåŒºé—´ [{EP_START + active_start_rel} -> {EP_START + active_end_rel}]")
    print(f"ğŸ“¸ æœ€ç»ˆæçº¯çš„ 9 ä¸ªå…³é”®å¸§ç´¢å¼•: {indices}")

    combo_images = []
    for i, idx in enumerate(indices):
        frame = reader.get_frame(idx)
        if frame and hasattr(frame, 'images') and frame.images:
            combo_arr = create_optimal_composite_frame(frame.images, step_num=i+1)
            combo_images.append(combo_arr)
            print(f"ğŸ–¼ï¸ æˆåŠŸæ¸²æŸ“åŠ¨ä½œèŠ‚ç‚¹: æ­¥éª¤ {i+1} (åŸå§‹å¸§ {idx})")
            
    reader.close()
    
    if len(combo_images) == 9:
        row1 = np.hstack(combo_images[0:3])
        row2 = np.hstack(combo_images[3:6])
        row3 = np.hstack(combo_images[6:9])
        master_grid = np.vstack((row1, row2, row3))
        
        max_dim = max(master_grid.shape[0], master_grid.shape[1])
        if max_dim > 3000:
            scale = 3000 / max_dim
            master_grid = cv2.resize(master_grid, (0,0), fx=scale, fy=scale)
            
        output_file = "task_test_optimal_grid.jpg"
        Image.fromarray(master_grid).save(output_file, quality=90)
        print(f"\nğŸ‰ å®Œç¾ï¼K-Meansç‰©ç†å¯¹é½æ’ç‰ˆå·²ç”Ÿæˆ: {output_file}")
    
    # ==========================================
    # ğŸŒŸğŸŒŸ æ ¸å¿ƒé›†æˆï¼šå¡«å…¥ä½ ä»ç½‘é¡µç«¯è·å–çš„ JSON
    # ==========================================
    print("\n[Mock] æ­£åœ¨æ¥æ”¶ VLM ç½‘é¡µç«¯è¿”å›çš„è¯­ä¹‰æŒ‡ä»¤...")
    mock_vlm_json = [
        {
            "subtask_id": 1,
            "instruction": "Left hand approaches and grasps the phone on the left",
            "start_image": 1,
            "end_image": 2
        },
        {
            "subtask_id": 2,
            "instruction": "Left hand lifts the phone, moves it to the center, and places it on the black base",
            "start_image": 2,
            "end_image": 5
        },
        {
            "subtask_id": 3,
            "instruction": "Right hand approaches and grasps the handset of the phone.",
            "start_image": 5,
            "end_image": 6
        },
        {
            "subtask_id": 4,
            "instruction": "Right hand lifts the handset off the base.",
            "start_image": 6,
            "end_image": 7
        },
        {
            "subtask_id": 5,
            "instruction": "Right hand moves the handset to the right and places it on the table.",
            "start_image": 7,
            "end_image": 9
        }
    ]

    # æ‰§è¡Œç‰©ç†ç¼åˆï¼Œä¼ å…¥åœ¨ä¸»å‡½æ•°é¡¶éƒ¨é…ç½®å¥½çš„ç¡¬ä»¶å‚æ•°
    final_labels = align_and_segment(
        mock_vlm_json, 
        indices_rel, 
        qpos_data, 
        dataset_start_offset=EP_START,
        gripper_dim_indices=ROBOT_CONFIG["gripper_dim_indices"],
        gripper_threshold=ROBOT_CONFIG["gripper_threshold"]
    )

    print("\n===========================================")
    print("ğŸ† æ­å–œï¼Robo-ETL æœ€ç»ˆè¾“å‡ºæ•°æ®é›†å¯ç”¨çš„æ ‡å‡†æ ¼å¼ï¼š")
    
    clean_output = [
        {
            "subtask_id": r["subtask_id"], 
            "instruction": r["instruction"], 
            "start_frame": r["global_start_frame"], 
            "end_frame": r["global_end_frame"]
        } for r in final_labels
    ]
    print(json.dumps(clean_output, indent=2, ensure_ascii=False))
    print("===========================================\n")

if __name__ == "__main__":
    main()