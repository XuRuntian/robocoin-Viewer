import torch
import numpy as np
from transformers import CLIPProcessor, CLIPModel
from sklearn.metrics.pairwise import cosine_similarity
from src.core.factory import ReaderFactory
from PIL import Image
import os

class AIScreener:
    """
    åŸºäºCLIPæ¨¡å‹çš„AIæ•°æ®ç­›æŸ¥å™¨
    åŠŸèƒ½ï¼šé€šè¿‡æå–æ•°æ®é›†ä¸­é—´å¸§çš„ç‰¹å¾å‘é‡ï¼Œä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æ£€æµ‹ç¦»ç¾¤æ•°æ®
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ¨¡å‹ã€å¤„ç†å™¨å’Œè®¾å¤‡é…ç½®"""
        # è‡ªåŠ¨æ£€æµ‹è®¡ç®—è®¾å¤‡
        self.device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
        
        # åŠ è½½CLIPæ¨¡å‹å’Œå¤„ç†å™¨ï¼ˆä½¿ç”¨vit-base-patch32æ¶æ„ï¼‰
        self.model_name = "openai/clip-vit-base-patch32"
        self.model = CLIPModel.from_pretrained(self.model_name).to(self.device)
        self.processor = CLIPProcessor.from_pretrained(self.model_name)

    def _get_image_from_dataset(self, path: str):
        """
        ä»æ•°æ®é›†ä¸­æå–ä¸­é—´å¸§å›¾åƒ
        å‚æ•°:
            path: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        è¿”å›:
            PIL.Image.Image å¯¹è±¡æˆ– Noneï¼ˆå¤±è´¥æ—¶ï¼‰
        """
        try:
            # è·å–æ•°æ®é›†è¯»å–å™¨
            reader = ReaderFactory.get_reader(path)
            if not reader:
                raise ValueError(f"æ— æ³•è¯†åˆ«æ–‡ä»¶æ ¼å¼: {path}")
                
            # åŠ è½½æ•°æ®é›†å¹¶è·å–ä¸­é—´å¸§
            reader.load(path)
            mid_idx = reader.get_length() // 2
            
            # æå–å›¾åƒæ•°æ®
            frame = reader.get_frame(mid_idx)
            
            # å…¼å®¹æ€§é˜²å¾¡ï¼šå¤„ç†ç©ºå¸§æˆ–è§£ç å¤±è´¥çš„æƒ…å†µ
            if frame is None or not hasattr(frame, 'images') or not frame.images:
                print(f"âš ï¸ æ— æ³•ä» [{os.path.basename(path)}] æå–å›¾åƒ (å¯èƒ½ç”±äºæœåŠ¡å™¨ç¼ºå°‘è§†é¢‘è§£ç å™¨)")
                reader.close()
                return None
                
            images = frame.images
            
            # ä¼˜å…ˆé€‰æ‹©æŒ‡å®šè§†è§’çš„å›¾åƒ
            image = None
            for key in ['cam_high', 'front', 'image', 'camera']:
                if key in images:
                    image = images[key]
                    break
            
            # å¦‚æœæ²¡æ‰¾åˆ°æŒ‡å®šè§†è§’ï¼Œå–ç¬¬ä¸€ä¸ªå¯ç”¨å›¾åƒ
            if image is None and images:
                image = next(iter(images.values()))
                
            reader.close()
            
            # äºŒæ¬¡é˜²å¾¡
            if image is None:
                return None
            
            # ç¡®ä¿è¿”å›PILå›¾åƒå¯¹è±¡
            if not isinstance(image, Image.Image):
                image = Image.fromarray(image)
            return image.convert('RGB')
                
        except Exception as e:
            print(f"âŒ å›¾åƒè¯»å–å¤±è´¥ [{path}]: {str(e)}")
            return None

    def extract_embeddings(self, dataset_paths: list):
        """
        æå–æ•°æ®é›†ä¸­é—´å¸§çš„CLIPç‰¹å¾å‘é‡
        å‚æ•°:
            dataset_paths: æ•°æ®é›†æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        è¿”å›:
            å­—å…¸ {è·¯å¾„: ç‰¹å¾å‘é‡}
        """
        embeddings = {}
        total = len(dataset_paths)
        
        for i, path in enumerate(dataset_paths):
            # è·å–å›¾åƒ
            image = self._get_image_from_dataset(path)
            if image is None:
                continue
                
            try:
                # ç‰¹å¾æå–
                inputs = self.processor(images=image, return_tensors="pt").to(self.device)
                with torch.no_grad():
                    outputs = self.model.get_image_features(**inputs)
                
                # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶å±•å¹³
                embeddings[path] = outputs.cpu().numpy().flatten()
                
                # è¿›åº¦æ˜¾ç¤º
                print(f"ğŸ§  [AI æå–ä¸­] {i+1}/{total}: {os.path.basename(path)}")
                
            except Exception as e:
                print(f"âŒ ç‰¹å¾æå–å¤±è´¥ [{path}]: {str(e)}")
                continue
                
        return embeddings

    def detect_outliers(self, dataset_paths: list, outlier_ratio=0.05, similarity_threshold=0.85):
        """
        ç¦»ç¾¤æ•°æ®æ£€æµ‹æ ¸å¿ƒæ–¹æ³•
        å‚æ•°:
            dataset_paths: æ•°æ®é›†æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            outlier_ratio: ç¦»ç¾¤æ¯”ä¾‹é˜ˆå€¼ï¼ˆé»˜è®¤5%ï¼‰
            similarity_threshold: ç›¸ä¼¼åº¦ç»å¯¹é˜ˆå€¼ï¼ˆé»˜è®¤0.85ï¼‰
        è¿”å›:
            å¯ç–‘è·¯å¾„åˆ—è¡¨
        """
        # æå–ç‰¹å¾å‘é‡
        features = self.extract_embeddings(dataset_paths)
        if len(features) < 3:
            print("âš ï¸ æ ·æœ¬æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œç¦»ç¾¤æ£€æµ‹")
            return []
            
        # ç‰¹å¾çŸ©é˜µæ„å»º
        feature_matrix = np.stack(list(features.values()))
        paths = np.array(list(features.keys()))
        
        # è®¡ç®—ä¸­å¿ƒå‘é‡
        centroid = np.mean(feature_matrix, axis=0)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = cosine_similarity(feature_matrix, [centroid]).flatten()
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼ˆä»å°åˆ°å¤§ï¼‰
        sorted_indices = np.argsort(similarities)
        
        # ç¦»ç¾¤åˆ¤å®šé€»è¾‘ï¼š
        # 1. é¦–å…ˆæŒ‰ç›¸ä¼¼åº¦æ’åºå–æœ€ä½çš„outlier_ratioæ¯”ä¾‹
        # 2. å†è¿‡æ»¤å‡ºç›¸ä¼¼åº¦ä½äºthresholdçš„æ ·æœ¬
        outlier_count = max(1, int(len(features) * outlier_ratio))
        suspect_indices = sorted_indices[:outlier_count]
        low_similarity_mask = similarities[suspect_indices] < similarity_threshold
        
        # æœ€ç»ˆå¯ç–‘æ ·æœ¬
        suspects = paths[suspect_indices][low_similarity_mask]
        
        # æ‰“å°æ£€æµ‹ç»“æœ
        print(f"\nğŸ” ç¦»ç¾¤æ£€æµ‹å®Œæˆ:")
        print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {len(features)}")
        print(f"ğŸ“‰ ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}")
        print(f"ğŸ¯ ç¦»ç¾¤æ¯”ä¾‹: {outlier_ratio*100}% ({outlier_count}ä¸ª)")
        print(f"ğŸš¨ æ£€æµ‹åˆ°å¯ç–‘æ ·æœ¬: {len(suspects)} ä¸ª")
        print("\n".join([f" - {os.path.basename(p)} (ç›¸ä¼¼åº¦: {similarities[i]:.3f})" 
                        for i, p in zip(suspect_indices[low_similarity_mask], suspects)]))
        
        return list(suspects)

if __name__ == "__main__":
    # æµ‹è¯•ç¤ºä¾‹
    screener = AIScreener()
    test_paths = [
        "data/valid/episode_0.hdf5",
        "data/valid/episode_1.hdf5",
        "data/valid/episode_2.hdf5",
        "data/valid/episode_3.hdf5",
        "data/valid/episode_4.hdf5",
        "data/valid/episode_5.hdf5",
        "data/valid/episode_6.hdf5",
        "data/valid/episode_7.hdf5",
        "data/valid/episode_8.hdf5",
        "data/valid/episode_9.hdf5"
    ]
    outliers = screener.detect_outliers(test_paths)
    print("\nâœ… æœ€ç»ˆå¯ç–‘æ•°æ®è·¯å¾„:")
    for path in outliers:
        print(path)
