# src/core/inspector.py
import os
from pathlib import Path
from collections import defaultdict
from src.core.factory import ReaderFactory
import pandas as pd

class DatasetInspector:
    def __init__(self, root_dir: str):
        self.root = Path(root_dir)
        self.report = []
        self.stats = defaultdict(int)
        self.grouped_datasets = defaultdict(list)  # æŒ‰ç±»å‹å­˜å‚¨æœ‰æ•ˆæ•°æ®é›†
        self.dominant_type = None

    def scan(self):
        print(f"ğŸ•µï¸â€â™‚ï¸ æ­£åœ¨æ‰«æç›®å½•: {self.root}")
        
        # ä½¿ç”¨ os.walk è¿›è¡Œé€’å½’æ‰«æ
        for root_path, dirs, files in os.walk(self.root):
            current_path = Path(root_path)
            
            # è·³è¿‡éšè—ç›®å½•
            if any(part.startswith('.') for part in current_path.parts):
                continue
                
            # æ£€æŸ¥å½“å‰ç›®å½•æ˜¯å¦ä¸ºæœ‰æ•ˆæ•°æ®é›†
            dtype = ReaderFactory.detect_type(current_path)
            
            # å¦‚æœæ˜¯æœ‰æ•ˆæ•°æ®é›†ï¼ˆé Unknown ä¸”é RawFolderï¼‰
            if dtype not in ("Unknown", "RawFolder"):
                self.stats[dtype] += 1
                self._add_record(current_path, dtype)
                
                # è·³è¿‡å¯¹è¯¥ç›®å½•å†…å®¹çš„è¿›ä¸€æ­¥é€’å½’
                dirs[:] = []
                
            else:
                # å¦‚æœç›®å½•æœ¬èº«ä¸æ˜¯æ•°æ®é›†ï¼Œåˆ™æ£€æŸ¥é‡Œé¢çš„æ–‡ä»¶ (é’ˆå¯¹ HDF5, ROS ç­‰å•æ–‡ä»¶æ ¼å¼)
                for f in files:
                    if f.startswith("."): continue
                    
                    file_path = current_path / f
                    file_dtype = ReaderFactory.detect_type(file_path)
                    
                    if file_dtype not in ("Unknown", "RawFolder"):
                        self.stats[file_dtype] += 1
                        self._add_record(file_path, file_dtype)

    def _add_record(self, path, dtype):
        info = {
            "name": path.name,
            "path": str(path),
            "type": dtype,
            "status": "OK"
        }
        self.report.append(info)
        self.grouped_datasets[dtype].append(str(path))

    def check_consistency(self) -> bool:
        """
        æ”¾å®½çš„æ£€æŸ¥é€»è¾‘ - å…è®¸æ··åˆç±»å‹
        """
        print("\n" + "="*40)
        print("ğŸ” é˜¶æ®µä¸€ï¼šæ ¼å¼ä¸€è‡´æ€§æ£€æŸ¥")
        print("="*40)
        
        # 1. æ£€æŸ¥æ˜¯å¦æœ‰ Unknown
        if self.stats["Unknown"] > 0:
            print(f"âŒ å¤±è´¥: åŒ…å« {self.stats['Unknown']} ä¸ªæœªçŸ¥æ ¼å¼çš„æ–‡ä»¶/æ–‡ä»¶å¤¹ã€‚")
            self._print_problems()
            return False

        valid_types = [t for t in self.stats.keys() if t != "Unknown"]
        if len(valid_types) == 0:
            print("âŒ å¤±è´¥: ç›®å½•ä¸‹æ²¡æœ‰æœ‰æ•ˆæ•°æ®ã€‚")
            return False

        print(f"âœ… é€šè¿‡: ç›®å½•ä¸‹å…± {sum(len(v) for v in self.grouped_datasets.values())} ä¸ªæ•°æ®ï¼ŒåŒ…å«ç±»å‹: {valid_types}")
        return True

    def _print_problems(self):
        df = pd.DataFrame(self.report)
        problems = df[df['status'].str.contains("Unknown|Corrupt|âŒ|âš ï¸")]
        if not problems.empty:
            print("\nğŸš¨ é—®é¢˜æ•°æ®æ¸…å•:")
            print(problems[['name', 'type', 'status']].to_markdown(index=False))

    def get_all_valid_paths(self):
        all_paths = []
        for paths in self.grouped_datasets.values():
            all_paths.extend(paths)
        return sorted(all_paths)
